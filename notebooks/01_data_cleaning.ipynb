{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a57187",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparation for MLflow Demo\n",
    "\n",
    "This notebook demonstrates the data cleaning and preparation process for the MLflow demo project. We'll perform the following steps:\n",
    "\n",
    "1. Set up the environment and MLflow tracking with DagsHub\n",
    "2. Load data from scikit-learn datasets\n",
    "3. Explore and analyze the data\n",
    "4. Preprocess the data (scaling, handling outliers, etc.)\n",
    "5. Split the data into training and testing sets\n",
    "6. Save raw and processed data\n",
    "7. Create baseline statistics for drift detection\n",
    "8. Log data preprocessing steps to MLflow\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.11+\n",
    "- Required packages (scikit-learn, pandas, numpy, matplotlib, mlflow, dagshub)\n",
    "- DagsHub account and repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48194d26",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "\n",
    "This notebook demonstrates data cleaning and preprocessing for the MLflow demo project.\n",
    "We'll work with the wine classification dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3ae04",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5352828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wine dataset\n",
    "wine = load_wine()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "df['target'] = wine.target\n",
    "df['target_name'] = df['target'].map({i: name for i, name in enumerate(wine.target_names)})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Features: {len(wine.feature_names)}\")\n",
    "print(f\"Classes: {wine.target_names}\")\n",
    "print(f\"Samples per class: {df['target'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a42b908",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee61fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "print(\"\\nBasic Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bf43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e01edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['target'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Class Distribution (Numeric)')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['target_name'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Class Distribution (Names)')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fdc045",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b321c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation matrix\n",
    "plt.figure(figsize=(16, 12))\n",
    "correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions by class\n",
    "numerical_features = wine.feature_names[:6]  # First 6 features for visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    for target in df['target'].unique():\n",
    "        subset = df[df['target'] == target][feature]\n",
    "        axes[i].hist(subset, alpha=0.7, label=f'Class {target}', bins=20)\n",
    "    \n",
    "    axes[i].set_title(f'{feature} Distribution by Class')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b918eaa",
   "metadata": {},
   "source": [
    "## 4. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b464e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "numerical_features = wine.feature_names[:8]  # First 8 features\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    axes[i].boxplot(df[feature])\n",
    "    axes[i].set_title(f'{feature} - Outliers')\n",
    "    axes[i].set_ylabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca135ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers using IQR method\n",
    "def identify_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Check outliers for each numerical feature\n",
    "outlier_summary = {}\n",
    "for feature in wine.feature_names:\n",
    "    outliers, lower, upper = identify_outliers(df, feature)\n",
    "    outlier_summary[feature] = {\n",
    "        'count': len(outliers),\n",
    "        'percentage': len(outliers) / len(df) * 100,\n",
    "        'bounds': (lower, upper)\n",
    "    }\n",
    "\n",
    "# Display outlier summary\n",
    "outlier_df = pd.DataFrame(outlier_summary).T\n",
    "outlier_df['count'] = outlier_df['count'].astype(int)\n",
    "outlier_df['percentage'] = outlier_df['percentage'].round(2)\n",
    "print(\"Outlier Summary:\")\n",
    "print(outlier_df[['count', 'percentage']].sort_values('count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a3867",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d4f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Remove outliers using IQR method (conservative approach)\n",
    "features_to_clean = ['total_phenols', 'flavanoids', 'proanthocyanins']  # Features with most outliers\n",
    "\n",
    "for feature in features_to_clean:\n",
    "    Q1 = df_cleaned[feature].quantile(0.25)\n",
    "    Q3 = df_cleaned[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    initial_count = len(df_cleaned)\n",
    "    df_cleaned = df_cleaned[(df_cleaned[feature] >= lower_bound) & (df_cleaned[feature] <= upper_bound)]\n",
    "    removed_count = initial_count - len(df_cleaned)\n",
    "    \n",
    "    print(f\"Removed {removed_count} outliers from {feature}\")\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df_cleaned.shape}\")\n",
    "print(f\"Data retention rate: {len(df_cleaned)/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62d129",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c6d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "df_engineered = df_cleaned.copy()\n",
    "\n",
    "# Ratio features\n",
    "df_engineered['alcohol_to_acidity_ratio'] = df_engineered['alcohol'] / (df_engineered['total_acidity'] + 1e-6)\n",
    "df_engineered['phenols_to_flavanoids_ratio'] = df_engineered['total_phenols'] / (df_engineered['flavanoids'] + 1e-6)\n",
    "\n",
    "# Polynomial features (squares)\n",
    "df_engineered['alcohol_squared'] = df_engineered['alcohol'] ** 2\n",
    "df_engineered['flavanoids_squared'] = df_engineered['flavanoids'] ** 2\n",
    "\n",
    "# Interaction features\n",
    "df_engineered['alcohol_flavanoids_interaction'] = df_engineered['alcohol'] * df_engineered['flavanoids']\n",
    "df_engineered['phenols_proanthocyanins_interaction'] = df_engineered['total_phenols'] * df_engineered['proanthocyanins']\n",
    "\n",
    "# Binned features\n",
    "df_engineered['alcohol_level'] = pd.cut(df_engineered['alcohol'], \n",
    "                                       bins=[0, 11, 12.5, 15], \n",
    "                                       labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(f\"Original features: {len(wine.feature_names)}\")\n",
    "new_features = [col for col in df_engineered.columns if col not in df.columns]\n",
    "print(f\"New features created: {len(new_features)}\")\n",
    "print(f\"New features: {new_features}\")\n",
    "print(f\"Total features: {len(df_engineered.columns) - 2}\")  # Excluding target columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b00caf",
   "metadata": {},
   "source": [
    "## 7. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee381dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for scaling\n",
    "feature_columns = [col for col in df_engineered.columns \n",
    "                  if col not in ['target', 'target_name', 'alcohol_level']]\n",
    "\n",
    "X = df_engineered[feature_columns]\n",
    "y = df_engineered['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Class distribution in training set: {pd.Series(y_train).value_counts().sort_index()}\")\n",
    "print(f\"Class distribution in test set: {pd.Series(y_test).value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"Scaling completed!\")\n",
    "print(f\"Scaled training set mean: {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"Scaled training set std: {X_train_scaled.std().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c9b39b",
   "metadata": {},
   "source": [
    "## 8. Visualization of Scaled Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs scaled features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "features_to_show = X_train.columns[:6]\n",
    "\n",
    "for i, feature in enumerate(features_to_show):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    # Original feature\n",
    "    axes[row, col].hist(X_train[feature], alpha=0.7, label='Original', bins=20)\n",
    "    # Scaled feature\n",
    "    axes[row, col].hist(X_train_scaled[feature], alpha=0.7, label='Scaled', bins=20)\n",
    "    \n",
    "    axes[row, col].set_title(f'{feature}')\n",
    "    axes[row, col].legend()\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b8d54",
   "metadata": {},
   "source": [
    "## 9. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a94495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directories\n",
    "import os\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save raw data\n",
    "df.to_csv('../data/raw/wine_dataset.csv', index=False)\n",
    "print(\"Raw data saved to ../data/raw/wine_dataset.csv\")\n",
    "\n",
    "# Save processed training data\n",
    "train_processed = X_train_scaled.copy()\n",
    "train_processed['target'] = y_train\n",
    "train_processed.to_csv('../data/processed/train_processed.csv', index=False)\n",
    "print(\"Processed training data saved to ../data/processed/train_processed.csv\")\n",
    "\n",
    "# Save processed test data\n",
    "test_processed = X_test_scaled.copy()\n",
    "test_processed['target'] = y_test\n",
    "test_processed.to_csv('../data/processed/test_processed.csv', index=False)\n",
    "print(\"Processed test data saved to ../data/processed/test_processed.csv\")\n",
    "\n",
    "# Save basic train/test splits (unscaled, for drift detection)\n",
    "train_basic = X_train.copy()\n",
    "train_basic['target'] = y_train\n",
    "train_basic.to_csv('../data/processed/wine_train.csv', index=False)\n",
    "\n",
    "test_basic = X_test.copy()\n",
    "test_basic['target'] = y_test\n",
    "test_basic.to_csv('../data/processed/wine_test.csv', index=False)\n",
    "\n",
    "print(\"\\nData preprocessing completed successfully!\")\n",
    "print(f\"Final dataset shape: {train_processed.shape[0] + test_processed.shape[0]} samples, {len(feature_columns)} features\")\n",
    "print(f\"Training samples: {train_processed.shape[0]}\")\n",
    "print(f\"Test samples: {test_processed.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcddc5c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Loaded** the wine dataset from sklearn\n",
    "2. **Explored** the data structure and distributions\n",
    "3. **Identified** and handled outliers\n",
    "4. **Engineered** new features (ratios, polynomials, interactions)\n",
    "5. **Scaled** features using StandardScaler\n",
    "6. **Split** data into training and test sets\n",
    "7. **Saved** processed data for use in subsequent steps\n",
    "\n",
    "The data is now ready for model training and drift detection analysis.\n",
    "\n",
    "**Next steps:**\n",
    "- Proceed to `02_drift.ipynb` for drift detection analysis\n",
    "- Use `03_model_training.ipynb` for model training experiments\n",
    "- Run the complete pipeline with `python main.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e64bd",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up MLflow tracking with DagsHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# For visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Add parent directory to path to import project modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Check if the directories exist, if not create them\n",
    "data_dir = os.path.join('..', 'data')\n",
    "raw_dir = os.path.join(data_dir, 'raw')\n",
    "processed_dir = os.path.join(data_dir, 'processed')\n",
    "drift_baseline_dir = os.path.join(data_dir, 'drift_baseline')\n",
    "\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "os.makedirs(drift_baseline_dir, exist_ok=True)\n",
    "\n",
    "print(\"Environment setup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be233a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MLflow tracking with DagsHub\n",
    "try:\n",
    "    import mlflow\n",
    "    import dagshub\n",
    "    \n",
    "    # Initialize DagsHub with MLflow tracking\n",
    "    dagshub.init(\n",
    "        repo_owner='yahiaehab10', \n",
    "        repo_name='MLflow_demo_MF', \n",
    "        mlflow=True\n",
    "    )\n",
    "    \n",
    "    # Set experiment name\n",
    "    mlflow.set_experiment(\"data_preparation\")\n",
    "    \n",
    "    print(\"MLflow tracking with DagsHub initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not initialize DagsHub MLflow tracking: {e}\")\n",
    "    print(\"Continuing with default MLflow tracking.\")\n",
    "    import mlflow\n",
    "    mlflow.set_experiment(\"data_preparation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2cb33",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Let's load a dataset from scikit-learn. For this demo, we'll use the diabetes dataset, which is a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0bb9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset from sklearn\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Create dataframes for features and target\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = pd.Series(diabetes.target)\n",
    "\n",
    "# Save dataset information for reference\n",
    "dataset_info = {\n",
    "    'name': 'diabetes',\n",
    "    'description': diabetes.DESCR,\n",
    "    'feature_names': diabetes.feature_names,\n",
    "    'target_names': None  # Diabetes dataset doesn't have target names\n",
    "}\n",
    "\n",
    "# Let's look at the data\n",
    "print(f\"Dataset: {dataset_info['name']}\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(\"\\nFeature names:\")\n",
    "for feature in diabetes.feature_names:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "# Save raw data\n",
    "X.to_csv(os.path.join(raw_dir, \"diabetes_features.csv\"), index=False)\n",
    "y.to_csv(os.path.join(raw_dir, \"diabetes_target.csv\"), index=False)\n",
    "joblib.dump(dataset_info, os.path.join(raw_dir, \"diabetes_info.joblib\"))\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name=\"data_loading\"):\n",
    "    mlflow.log_param(\"dataset\", \"diabetes\")\n",
    "    mlflow.log_param(\"data_shape\", X.shape)\n",
    "    mlflow.log_param(\"feature_count\", X.shape[1])\n",
    "    mlflow.log_param(\"sample_count\", X.shape[0])\n",
    "    \n",
    "    # Log dataset description as a text artifact\n",
    "    with open(os.path.join(raw_dir, \"dataset_description.txt\"), \"w\") as f:\n",
    "        f.write(diabetes.DESCR)\n",
    "    mlflow.log_artifact(os.path.join(raw_dir, \"dataset_description.txt\"))\n",
    "    \n",
    "    print(\"Data loaded and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32f488",
   "metadata": {},
   "source": [
    "## 3. Data Exploration\n",
    "\n",
    "Let's explore the dataset to understand its characteristics, check for missing values, and analyze distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics for features\n",
    "print(\"Feature Summary Statistics:\")\n",
    "print(X.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(X.isnull().sum())\n",
    "\n",
    "# Display histogram of target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y, bins=30, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of Target Variable (Disease Progression)')\n",
    "plt.xlabel('Disease Progression')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig(os.path.join(raw_dir, \"target_distribution.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Distribution of features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(X.columns):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    sns.histplot(X[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(raw_dir, \"feature_distributions.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = X.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.savefig(os.path.join(raw_dir, \"correlation_matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Check for outliers using box plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(X.columns):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    sns.boxplot(y=X[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(raw_dir, \"feature_boxplots.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Log exploratory analysis to MLflow\n",
    "with mlflow.start_run(run_name=\"data_exploration\"):\n",
    "    # Log statistics as parameters\n",
    "    for col in X.columns:\n",
    "        mlflow.log_param(f\"mean_{col}\", X[col].mean())\n",
    "        mlflow.log_param(f\"std_{col}\", X[col].std())\n",
    "    \n",
    "    # Log correlation with target as metrics\n",
    "    for col in X.columns:\n",
    "        correlation = np.corrcoef(X[col], y)[0, 1]\n",
    "        mlflow.log_metric(f\"target_corr_{col}\", correlation)\n",
    "    \n",
    "    # Log visualization artifacts\n",
    "    mlflow.log_artifact(os.path.join(raw_dir, \"target_distribution.png\"))\n",
    "    mlflow.log_artifact(os.path.join(raw_dir, \"feature_distributions.png\"))\n",
    "    mlflow.log_artifact(os.path.join(raw_dir, \"correlation_matrix.png\"))\n",
    "    mlflow.log_artifact(os.path.join(raw_dir, \"feature_boxplots.png\"))\n",
    "    \n",
    "    # Save and log correlation matrix as CSV\n",
    "    correlation_df = pd.DataFrame(correlation_matrix)\n",
    "    correlation_df.to_csv(os.path.join(raw_dir, \"correlation_matrix.csv\"))\n",
    "    mlflow.log_artifact(os.path.join(raw_dir, \"correlation_matrix.csv\"))\n",
    "    \n",
    "    print(\"Data exploration completed and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec4685",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Now let's preprocess the data:\n",
    "1. Handle any outliers\n",
    "2. Scale the features\n",
    "3. Create a preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf422a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a preprocessing pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Function to detect outliers using IQR\n",
    "def detect_outliers(df, n_std=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using standard deviation method\n",
    "    \"\"\"\n",
    "    data_clean = df.copy()\n",
    "    outliers_dict = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Calculate mean and standard deviation\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        \n",
    "        # Find outliers\n",
    "        outliers = df[(df[col] < mean - n_std * std) | (df[col] > mean + n_std * std)][col]\n",
    "        outliers_dict[col] = len(outliers)\n",
    "        \n",
    "        # Replace outliers with NaN (to be imputed later)\n",
    "        data_clean.loc[(data_clean[col] < mean - n_std * std) | (data_clean[col] > mean + n_std * std), col] = np.nan\n",
    "    \n",
    "    return data_clean, outliers_dict\n",
    "\n",
    "# Detect and handle outliers\n",
    "X_clean, outliers_dict = detect_outliers(X, n_std=3)\n",
    "print(\"Outliers detected in each feature:\")\n",
    "for col, count in outliers_dict.items():\n",
    "    print(f\"{col}: {count} outliers\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "scaler_type = 'standard'  # Can be 'standard' or 'minmax'\n",
    "\n",
    "steps = []\n",
    "# Add imputer to handle missing values and potential NaNs from outlier removal\n",
    "steps.append(('imputer', SimpleImputer(strategy='median')))\n",
    "\n",
    "# Add scaler\n",
    "if scaler_type == 'standard':\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "elif scaler_type == 'minmax':\n",
    "    steps.append(('scaler', MinMaxScaler()))\n",
    "else:\n",
    "    raise ValueError(f\"Scaler type {scaler_type} not supported.\")\n",
    "\n",
    "preprocessing_pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline on the data and transform it\n",
    "X_processed = pd.DataFrame(\n",
    "    preprocessing_pipeline.fit_transform(X_clean),\n",
    "    columns=X_clean.columns\n",
    ")\n",
    "\n",
    "# Show the processed data\n",
    "print(\"\\nProcessed Data (first 5 rows):\")\n",
    "print(X_processed.head())\n",
    "\n",
    "# Save the preprocessing pipeline\n",
    "preprocessing_pipeline_path = os.path.join(processed_dir, \"preprocessing_pipeline.joblib\")\n",
    "joblib.dump(preprocessing_pipeline, preprocessing_pipeline_path)\n",
    "\n",
    "# Log preprocessing to MLflow\n",
    "with mlflow.start_run(run_name=\"data_preprocessing\"):\n",
    "    # Log preprocessing parameters\n",
    "    mlflow.log_param(\"scaler_type\", scaler_type)\n",
    "    mlflow.log_param(\"imputer_strategy\", \"median\")\n",
    "    mlflow.log_param(\"outlier_handling\", \"replace_with_median\")\n",
    "    \n",
    "    # Log outlier counts as metrics\n",
    "    for col, count in outliers_dict.items():\n",
    "        mlflow.log_metric(f\"outliers_{col}\", count)\n",
    "    \n",
    "    # Log processed data statistics\n",
    "    for col in X_processed.columns:\n",
    "        mlflow.log_metric(f\"processed_mean_{col}\", X_processed[col].mean())\n",
    "        mlflow.log_metric(f\"processed_std_{col}\", X_processed[col].std())\n",
    "    \n",
    "    # Log preprocessing pipeline as artifact\n",
    "    mlflow.log_artifact(preprocessing_pipeline_path)\n",
    "    \n",
    "    # Compare original vs processed data with a visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, col in enumerate(X.columns):\n",
    "        plt.subplot(3, 4, i+1)\n",
    "        plt.hist(X[col], bins=20, alpha=0.5, label='Original')\n",
    "        plt.hist(X_processed[col], bins=20, alpha=0.5, label='Processed')\n",
    "        plt.title(f'{col}')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save and log the comparison\n",
    "    comparison_path = os.path.join(processed_dir, \"original_vs_processed.png\")\n",
    "    plt.savefig(comparison_path)\n",
    "    mlflow.log_artifact(comparison_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Preprocessing completed and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6298e3",
   "metadata": {},
   "source": [
    "## 5. Data Splitting\n",
    "\n",
    "Now we'll split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59429d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=test_size, random_state=random_state\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Save the split data\n",
    "X_train.to_csv(os.path.join(processed_dir, \"diabetes_X_train.csv\"), index=False)\n",
    "X_test.to_csv(os.path.join(processed_dir, \"diabetes_X_test.csv\"), index=False)\n",
    "y_train.to_csv(os.path.join(processed_dir, \"diabetes_y_train.csv\"), index=False)\n",
    "y_test.to_csv(os.path.join(processed_dir, \"diabetes_y_test.csv\"), index=False)\n",
    "\n",
    "# Log data splitting to MLflow\n",
    "with mlflow.start_run(run_name=\"data_splitting\"):\n",
    "    mlflow.log_param(\"test_size\", test_size)\n",
    "    mlflow.log_param(\"random_state\", random_state)\n",
    "    mlflow.log_param(\"train_size\", X_train.shape[0])\n",
    "    mlflow.log_param(\"test_size\", X_test.shape[0])\n",
    "    \n",
    "    # Log train/test split sizes\n",
    "    mlflow.log_metric(\"train_samples\", X_train.shape[0])\n",
    "    mlflow.log_metric(\"test_samples\", X_test.shape[0])\n",
    "    \n",
    "    # Log training and testing data paths\n",
    "    split_info = {\n",
    "        \"X_train_path\": os.path.join(processed_dir, \"diabetes_X_train.csv\"),\n",
    "        \"X_test_path\": os.path.join(processed_dir, \"diabetes_X_test.csv\"),\n",
    "        \"y_train_path\": os.path.join(processed_dir, \"diabetes_y_train.csv\"),\n",
    "        \"y_test_path\": os.path.join(processed_dir, \"diabetes_y_test.csv\")\n",
    "    }\n",
    "    \n",
    "    # Save split info as JSON\n",
    "    import json\n",
    "    with open(os.path.join(processed_dir, \"split_info.json\"), \"w\") as f:\n",
    "        json.dump(split_info, f)\n",
    "    \n",
    "    mlflow.log_artifact(os.path.join(processed_dir, \"split_info.json\"))\n",
    "    \n",
    "    print(\"Data splitting completed and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81da8036",
   "metadata": {},
   "source": [
    "## 6. Create Baseline for Drift Detection\n",
    "\n",
    "We'll create a baseline of the current data statistics that can be used later to detect data drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline statistics for drift detection\n",
    "baseline_stats = {\n",
    "    'mean': X_train.mean().to_dict(),\n",
    "    'std': X_train.std().to_dict(),\n",
    "    'min': X_train.min().to_dict(),\n",
    "    'max': X_train.max().to_dict(),\n",
    "    'median': X_train.median().to_dict(),\n",
    "    'shape': X_train.shape\n",
    "}\n",
    "\n",
    "# Save baseline statistics\n",
    "baseline_stats_path = os.path.join(drift_baseline_dir, \"diabetes_baseline_stats.joblib\")\n",
    "joblib.dump(baseline_stats, baseline_stats_path)\n",
    "\n",
    "# Also save a sample of the baseline data\n",
    "sample_size = min(1000, len(X_train))\n",
    "X_train_sample = X_train.sample(sample_size, random_state=42)\n",
    "X_train_sample.to_csv(os.path.join(drift_baseline_dir, \"diabetes_baseline_sample.csv\"), index=False)\n",
    "\n",
    "# Print baseline statistics\n",
    "print(\"Baseline Statistics:\")\n",
    "for stat_name, stat_values in baseline_stats.items():\n",
    "    if stat_name != 'shape':\n",
    "        print(f\"\\n{stat_name.capitalize()}:\")\n",
    "        for feature, value in stat_values.items():\n",
    "            print(f\"  {feature}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nShape: {stat_values}\")\n",
    "\n",
    "# Log drift baseline to MLflow\n",
    "with mlflow.start_run(run_name=\"drift_baseline_creation\"):\n",
    "    # Log baseline creation parameters\n",
    "    mlflow.log_param(\"baseline_source\", \"training_data\")\n",
    "    mlflow.log_param(\"baseline_sample_size\", sample_size)\n",
    "    \n",
    "    # Log baseline stats as metrics\n",
    "    for stat_name, stat_values in baseline_stats.items():\n",
    "        if stat_name != 'shape':\n",
    "            for feature, value in stat_values.items():\n",
    "                mlflow.log_metric(f\"baseline_{stat_name}_{feature}\", value)\n",
    "    \n",
    "    # Log baseline files as artifacts\n",
    "    mlflow.log_artifact(baseline_stats_path)\n",
    "    mlflow.log_artifact(os.path.join(drift_baseline_dir, \"diabetes_baseline_sample.csv\"))\n",
    "    \n",
    "    # Create and log distributions of baseline data\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, col in enumerate(X_train.columns):\n",
    "        plt.subplot(3, 4, i+1)\n",
    "        sns.histplot(X_train[col], kde=True)\n",
    "        plt.title(f'Baseline Distribution: {col}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    baseline_dist_path = os.path.join(drift_baseline_dir, \"baseline_distributions.png\")\n",
    "    plt.savefig(baseline_dist_path)\n",
    "    mlflow.log_artifact(baseline_dist_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Drift baseline created and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d0896",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In this notebook, we've:\n",
    "\n",
    "1. Set up the environment and MLflow tracking with DagsHub\n",
    "2. Loaded the diabetes dataset from scikit-learn\n",
    "3. Explored and analyzed the data\n",
    "4. Preprocessed the data (handled outliers and scaled features)\n",
    "5. Split the data into training and testing sets\n",
    "6. Created a baseline for drift detection\n",
    "7. Logged all steps and artifacts to MLflow\n",
    "\n",
    "All the processed data and artifacts are now available in the respective directories and can be used for model training and evaluation in the next notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ac42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of the work done\n",
    "print(\"Data Cleaning and Preparation Summary:\")\n",
    "print(f\"  - Dataset: diabetes\")\n",
    "print(f\"  - Original data shape: {X.shape}\")\n",
    "print(f\"  - Processed data shape: {X_processed.shape}\")\n",
    "print(f\"  - Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"  - Testing set size: {X_test.shape[0]} samples\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"  - Raw data: {os.path.join(raw_dir, 'diabetes_features.csv')}\")\n",
    "print(f\"  - Processed training data: {os.path.join(processed_dir, 'diabetes_X_train.csv')}\")\n",
    "print(f\"  - Processed testing data: {os.path.join(processed_dir, 'diabetes_X_test.csv')}\")\n",
    "print(f\"  - Preprocessing pipeline: {os.path.join(processed_dir, 'preprocessing_pipeline.joblib')}\")\n",
    "print(f\"  - Drift baseline: {os.path.join(drift_baseline_dir, 'diabetes_baseline_stats.joblib')}\")\n",
    "print(\"\\nMLflow experiments created:\")\n",
    "print(\"  - data_loading\")\n",
    "print(\"  - data_exploration\")\n",
    "print(\"  - data_preprocessing\")\n",
    "print(\"  - data_splitting\")\n",
    "print(\"  - drift_baseline_creation\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  - Proceed to drift detection notebook (02_drift.ipynb)\")\n",
    "print(\"  - Proceed to model training notebook (03_model_training.ipynb)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
