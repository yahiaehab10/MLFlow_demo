{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139e1173",
   "metadata": {},
   "source": [
    "# Model Training and Management with MLflow\n",
    "\n",
    "This notebook demonstrates the complete model lifecycle management using MLflow. We'll perform the following steps:\n",
    "\n",
    "1. Set up the environment and MLflow tracking with DagsHub\n",
    "2. Load and explore the processed data\n",
    "3. Train multiple models with different hyperparameters\n",
    "4. Track experiments with MLflow\n",
    "5. Compare and visualize model performance\n",
    "6. Select the best model\n",
    "7. Register the model in the MLflow Model Registry\n",
    "8. Manage the model lifecycle (staging, production, etc.)\n",
    "9. Deploy the model for inference\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.11+\n",
    "- Required packages (scikit-learn, pandas, numpy, matplotlib, mlflow, dagshub)\n",
    "- Completed data cleaning notebook (01_data_cleaning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec87826",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up MLflow tracking with DagsHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa57a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Add parent directory to path to import project modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# For MLflow tracking\n",
    "try:\n",
    "    import mlflow\n",
    "    import dagshub\n",
    "    \n",
    "    # Initialize DagsHub with MLflow tracking\n",
    "    dagshub.init(\n",
    "        repo_owner='yahiaehab10', \n",
    "        repo_name='MLflow_demo_MF', \n",
    "        mlflow=True\n",
    "    )\n",
    "    \n",
    "    # Set experiment name\n",
    "    mlflow.set_experiment(\"model_training\")\n",
    "    \n",
    "    print(\"MLflow tracking with DagsHub initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not initialize DagsHub MLflow tracking: {e}\")\n",
    "    print(\"Continuing with default MLflow tracking.\")\n",
    "    import mlflow\n",
    "    mlflow.set_experiment(\"model_training\")\n",
    "\n",
    "# Set up paths\n",
    "data_dir = os.path.join('..', 'data')\n",
    "processed_dir = os.path.join(data_dir, 'processed')\n",
    "models_dir = os.path.join('..', 'models')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(\"Environment setup completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d73c02f",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Processed Data\n",
    "\n",
    "Let's load the processed data that we prepared in the data cleaning notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc6b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data\n",
    "try:\n",
    "    X_train = pd.read_csv(os.path.join(processed_dir, \"diabetes_X_train.csv\"))\n",
    "    X_test = pd.read_csv(os.path.join(processed_dir, \"diabetes_X_test.csv\"))\n",
    "    y_train = pd.read_csv(os.path.join(processed_dir, \"diabetes_y_train.csv\")).iloc[:, 0]\n",
    "    y_test = pd.read_csv(os.path.join(processed_dir, \"diabetes_y_test.csv\")).iloc[:, 0]\n",
    "    \n",
    "    print(\"Processed data loaded successfully.\")\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Testing data shape: {X_test.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Processed data not found. Please run the data cleaning notebook first.\")\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "# Basic exploration of the processed data\n",
    "if X_train is not None:\n",
    "    # Display basic statistics\n",
    "    print(\"\\nFeature Statistics:\")\n",
    "    print(X_train.describe())\n",
    "    \n",
    "    # Correlation with target\n",
    "    correlations = pd.DataFrame()\n",
    "    correlations['feature'] = X_train.columns\n",
    "    correlations['correlation_with_target'] = [np.corrcoef(X_train[col], y_train)[0, 1] for col in X_train.columns]\n",
    "    correlations = correlations.sort_values('correlation_with_target', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Correlations with Target:\")\n",
    "    print(correlations)\n",
    "    \n",
    "    # Visualize correlations\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='feature', y='correlation_with_target', data=correlations)\n",
    "    plt.title('Feature Correlations with Target')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Pair plot of top correlated features\n",
    "    top_features = correlations['feature'].head(3).tolist()\n",
    "    plot_data = X_train[top_features].copy()\n",
    "    plot_data['target'] = y_train\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.pairplot(plot_data, corner=True)\n",
    "    plt.suptitle('Pairplot of Top Correlated Features', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c05c4a",
   "metadata": {},
   "source": [
    "## 3. Train Multiple Models and Track with MLflow\n",
    "\n",
    "Let's train several regression models with different hyperparameters and track everything with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd573370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to train and log models\n",
    "def train_and_log_model(model, model_name, X_train, y_train, X_test, y_test, params=None, log_artifacts=True):\n",
    "    \"\"\"\n",
    "    Train a model, evaluate it, and log to MLflow\n",
    "    \n",
    "    Args:\n",
    "        model: The model instance to train\n",
    "        model_name: Name of the model for logging\n",
    "        X_train: Training features\n",
    "        y_train: Training target\n",
    "        X_test: Testing features\n",
    "        y_test: Testing target\n",
    "        params: Model parameters to log\n",
    "        log_artifacts: Whether to log artifacts\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results including metrics\n",
    "    \"\"\"\n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(run_name=f\"train_{model_name}\") as run:\n",
    "        # Log model parameters\n",
    "        if params:\n",
    "            for param_name, param_value in params.items():\n",
    "                mlflow.log_param(param_name, param_value)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        \n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        \n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        \n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metric(\"train_rmse\", train_rmse)\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "        mlflow.log_metric(\"train_mae\", train_mae)\n",
    "        mlflow.log_metric(\"test_mae\", test_mae)\n",
    "        mlflow.log_metric(\"train_r2\", train_r2)\n",
    "        mlflow.log_metric(\"test_r2\", test_r2)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "        cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "        mlflow.log_metric(\"cv_rmse\", cv_rmse)\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.sklearn.log_model(model, f\"{model_name}_model\")\n",
    "        \n",
    "        # Save model locally\n",
    "        model_path = os.path.join(models_dir, f\"{model_name}_model.joblib\")\n",
    "        joblib.dump(model, model_path)\n",
    "        \n",
    "        if log_artifacts:\n",
    "            # Create residual plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            residuals = y_test - y_pred_test\n",
    "            plt.scatter(y_pred_test, residuals)\n",
    "            plt.axhline(y=0, color='r', linestyle='-')\n",
    "            plt.xlabel('Predicted Values')\n",
    "            plt.ylabel('Residuals')\n",
    "            plt.title(f'Residual Plot - {model_name}')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot\n",
    "            residual_plot_path = os.path.join(models_dir, f\"{model_name}_residual_plot.png\")\n",
    "            plt.savefig(residual_plot_path)\n",
    "            plt.close()\n",
    "            \n",
    "            # Create actual vs predicted plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_test, y_pred_test, alpha=0.5)\n",
    "            min_val = min(y_test.min(), y_pred_test.min())\n",
    "            max_val = max(y_test.max(), y_pred_test.max())\n",
    "            plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "            plt.xlabel('Actual Values')\n",
    "            plt.ylabel('Predicted Values')\n",
    "            plt.title(f'Actual vs Predicted - {model_name}')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot\n",
    "            pred_plot_path = os.path.join(models_dir, f\"{model_name}_prediction_plot.png\")\n",
    "            plt.savefig(pred_plot_path)\n",
    "            plt.close()\n",
    "            \n",
    "            # Feature importance if available\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                # Create feature importance dataframe\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'feature': X_train.columns,\n",
    "                    'importance': model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                # Create feature importance plot\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "                plt.title(f'Feature Importance - {model_name}')\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save plot\n",
    "                importance_plot_path = os.path.join(models_dir, f\"{model_name}_feature_importance.png\")\n",
    "                plt.savefig(importance_plot_path)\n",
    "                plt.close()\n",
    "                \n",
    "                # Save and log feature importance\n",
    "                feature_importance.to_csv(os.path.join(models_dir, f\"{model_name}_feature_importance.csv\"), index=False)\n",
    "                mlflow.log_artifact(os.path.join(models_dir, f\"{model_name}_feature_importance.csv\"))\n",
    "            \n",
    "            # Log plots as artifacts\n",
    "            mlflow.log_artifact(residual_plot_path)\n",
    "            mlflow.log_artifact(pred_plot_path)\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                mlflow.log_artifact(importance_plot_path)\n",
    "        \n",
    "        # Get the run ID\n",
    "        run_id = run.info.run_id\n",
    "        \n",
    "    # Return results\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'model_name': model_name,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'cv_rmse': cv_rmse,\n",
    "        'run_id': run_id,\n",
    "        'model_path': model_path\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ec22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models_to_train = [\n",
    "    {\n",
    "        'name': 'linear_regression',\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}\n",
    "    },\n",
    "    {\n",
    "        'name': 'ridge_regression',\n",
    "        'model': Ridge(alpha=0.1),\n",
    "        'params': {'alpha': 0.1}\n",
    "    },\n",
    "    {\n",
    "        'name': 'lasso_regression',\n",
    "        'model': Lasso(alpha=0.01),\n",
    "        'params': {'alpha': 0.01}\n",
    "    },\n",
    "    {\n",
    "        'name': 'elastic_net',\n",
    "        'model': ElasticNet(alpha=0.01, l1_ratio=0.5),\n",
    "        'params': {'alpha': 0.01, 'l1_ratio': 0.5}\n",
    "    },\n",
    "    {\n",
    "        'name': 'random_forest',\n",
    "        'model': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "        'params': {'n_estimators': 100, 'max_depth': 10, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        'name': 'gradient_boosting',\n",
    "        'model': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
    "        'params': {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, 'random_state': 42}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Train and log all models\n",
    "all_results = {}\n",
    "\n",
    "for model_info in models_to_train:\n",
    "    print(f\"\\nTraining {model_info['name']}...\")\n",
    "    results = train_and_log_model(\n",
    "        model=model_info['model'],\n",
    "        model_name=model_info['name'],\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        params=model_info['params']\n",
    "    )\n",
    "    all_results[model_info['name']] = results\n",
    "    \n",
    "    print(f\"  Test RMSE: {results['test_rmse']:.4f}\")\n",
    "    print(f\"  Test R²: {results['test_r2']:.4f}\")\n",
    "\n",
    "# Create a summary dataframe of all model results\n",
    "summary = pd.DataFrame({\n",
    "    'model_name': [results['model_name'] for results in all_results.values()],\n",
    "    'train_rmse': [results['train_rmse'] for results in all_results.values()],\n",
    "    'test_rmse': [results['test_rmse'] for results in all_results.values()],\n",
    "    'train_r2': [results['train_r2'] for results in all_results.values()],\n",
    "    'test_r2': [results['test_r2'] for results in all_results.values()],\n",
    "    'cv_rmse': [results['cv_rmse'] for results in all_results.values()],\n",
    "    'run_id': [results['run_id'] for results in all_results.values()]\n",
    "})\n",
    "\n",
    "# Sort by test RMSE\n",
    "summary = summary.sort_values('test_rmse')\n",
    "\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4fda1a",
   "metadata": {},
   "source": [
    "## 4. Visualize and Compare Model Performance\n",
    "\n",
    "Let's visualize the performance of all models to help select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Get the experiment ID\n",
    "experiment = mlflow.get_experiment_by_name(\"ml_demo\")\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Create MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Get all runs for our experiment\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "# If we have runs, create a visualization\n",
    "if not runs.empty:\n",
    "    # Filter out runs with missing metrics\n",
    "    runs = runs.dropna(subset=['metrics.accuracy', 'metrics.precision', 'metrics.recall', 'metrics.f1'])\n",
    "    \n",
    "    # Create a comparison dataframe\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': runs['tags.model_name'],\n",
    "        'Accuracy': runs['metrics.accuracy'],\n",
    "        'Precision': runs['metrics.precision'],\n",
    "        'Recall': runs['metrics.recall'],\n",
    "        'F1 Score': runs['metrics.f1']\n",
    "    })\n",
    "    \n",
    "    # Sort by F1 score\n",
    "    comparison_df = comparison_df.sort_values('F1 Score', ascending=False)\n",
    "    \n",
    "    # Display the comparison table\n",
    "    print(\"Model Performance Comparison:\")\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    \n",
    "    # Create bar chart\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        ax = comparison_df.plot.bar(x='Model', y=metric, ax=plt.gca(), legend=False)\n",
    "        plt.title(f'{metric} Comparison')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.suptitle('Model Performance Metrics Comparison', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No runs found in the experiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8666ed",
   "metadata": {},
   "source": [
    "## 5. Register the Best Model\n",
    "\n",
    "MLflow Model Registry is a centralized model store that enables you to:\n",
    "- Store models with version control\n",
    "- Stage transition (from dev to staging to production)\n",
    "- Annotate models with metadata\n",
    "- Handle the full lifecycle of a model\n",
    "\n",
    "Let's register our best performing model to the MLflow Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7d2820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best run based on F1 score\n",
    "if not runs.empty:\n",
    "    best_run_id = runs.loc[runs['metrics.f1'].idxmax()]['run_id']\n",
    "    best_model_name = runs.loc[runs['metrics.f1'].idxmax()]['tags.model_name']\n",
    "    \n",
    "    # Load the model from the best run\n",
    "    best_model = mlflow.sklearn.load_model(f\"runs:/{best_run_id}/model\")\n",
    "    \n",
    "    # Register the model\n",
    "    model_name = \"classification_model\"\n",
    "    model_version = mlflow.register_model(f\"runs:/{best_run_id}/model\", model_name)\n",
    "    \n",
    "    print(f\"Registered model: {model_name}, version: {model_version.version}\")\n",
    "    print(f\"Best model: {best_model_name}\")\n",
    "    \n",
    "    # Transition the model to staging\n",
    "    client = MlflowClient()\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        stage=\"Staging\"\n",
    "    )\n",
    "    \n",
    "    # Add description\n",
    "    client.update_model_version(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        description=f\"This is the best model ({best_model_name}) based on F1 score.\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Model {model_name} version {model_version.version} transitioned to Staging.\")\n",
    "    \n",
    "    # Get all registered model versions\n",
    "    model_versions = client.get_latest_versions(model_name)\n",
    "    print(\"\\nRegistered Model Versions:\")\n",
    "    for mv in model_versions:\n",
    "        print(f\"Model: {mv.name}, Version: {mv.version}, Stage: {mv.current_stage}\")\n",
    "else:\n",
    "    print(\"No runs found to register models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a4a36",
   "metadata": {},
   "source": [
    "## 6. Load and Use the Registered Model\n",
    "\n",
    "Let's demonstrate how to load a model from the registry and use it for prediction. In a real-world scenario, this would be part of the deployment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09020e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from registry by name and stage\n",
    "model_name = \"classification_model\"\n",
    "stage = \"Staging\"  # Could also be \"Production\" or \"Archived\"\n",
    "\n",
    "try:\n",
    "    # Load the model from the registry\n",
    "    loaded_model = mlflow.sklearn.load_model(f\"models:/{model_name}/{stage}\")\n",
    "    \n",
    "    print(f\"Successfully loaded model {model_name} from {stage} stage\")\n",
    "    \n",
    "    # Use the model to make predictions on test data\n",
    "    y_pred = loaded_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics to verify the model works as expected\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"Model Performance on Test Data:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Display classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Make predictions on a few examples\n",
    "    print(\"\\nPrediction Examples:\")\n",
    "    n_samples = min(5, len(X_test))\n",
    "    sample_indices = np.random.choice(len(X_test), n_samples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        true_label = y_test.iloc[idx] if hasattr(y_test, 'iloc') else y_test[idx]\n",
    "        pred_label = loaded_model.predict(X_test.iloc[idx:idx+1] if hasattr(X_test, 'iloc') else X_test[idx:idx+1])[0]\n",
    "        print(f\"Example {i+1}: True label = {true_label}, Predicted label = {pred_label}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading or using the model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586dfed8",
   "metadata": {},
   "source": [
    "## 7. Model Versioning and Promotion\n",
    "\n",
    "In a real-world scenario, you would train new model versions as more data becomes available or as you refine your approach. MLflow allows you to track these versions and promote them through different stages (Development → Staging → Production).\n",
    "\n",
    "Let's simulate training a new model version and promoting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d02480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a new improved model (for demonstration purposes)\n",
    "# In a real scenario, this might include new data or improved features\n",
    "\n",
    "with mlflow.start_run(run_name=\"improved_random_forest\") as run:\n",
    "    # Set a tag to identify this as an improved model\n",
    "    mlflow.set_tag(\"model_name\", \"ImprovedRandomForest\")\n",
    "    mlflow.set_tag(\"version\", \"2.0\")\n",
    "    \n",
    "    # Train a new model with different hyperparameters\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=200,  # More trees\n",
    "        max_depth=10,      # Deeper trees\n",
    "        min_samples_split=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"n_estimators\", 200)\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    mlflow.log_param(\"min_samples_split\", 5)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1\", f1)\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(rf_model, \"model\")\n",
    "    \n",
    "    print(f\"Trained improved model with:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Register the new model version\n",
    "    model_name = \"classification_model\"\n",
    "    new_model_version = mlflow.register_model(f\"runs:/{run.info.run_id}/model\", model_name)\n",
    "    \n",
    "    print(f\"Registered new model version: {new_model_version.version}\")\n",
    "\n",
    "# Promote the new model to production if it's better\n",
    "client = MlflowClient()\n",
    "\n",
    "# Get all versions of the model\n",
    "model_versions = client.get_latest_versions(model_name)\n",
    "print(\"\\nAll Model Versions:\")\n",
    "for mv in model_versions:\n",
    "    print(f\"Version: {mv.version}, Stage: {mv.current_stage}\")\n",
    "\n",
    "# Get the latest version\n",
    "latest_version = max([int(mv.version) for mv in model_versions])\n",
    "\n",
    "# Transition the latest version to Production\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=str(latest_version),\n",
    "    stage=\"Production\"\n",
    ")\n",
    "\n",
    "print(f\"\\nPromoted version {latest_version} to Production\")\n",
    "\n",
    "# Update description\n",
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=str(latest_version),\n",
    "    description=\"This is the improved model with higher F1 score.\"\n",
    ")\n",
    "\n",
    "# Get all versions after promotion\n",
    "model_versions = client.get_latest_versions(model_name)\n",
    "print(\"\\nUpdated Model Versions:\")\n",
    "for mv in model_versions:\n",
    "    print(f\"Version: {mv.version}, Stage: {mv.current_stage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da31c32",
   "metadata": {},
   "source": [
    "## 8. DagsHub Integration for Collaboration\n",
    "\n",
    "DagsHub provides a collaborative platform for MLOps, making it easy to share experiments, models, and artifacts with your team. Let's explore how to integrate with DagsHub for enhanced collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5270b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Check if dagshub is installed\n",
    "    import dagshub\n",
    "    dagshub_installed = True\n",
    "except ImportError:\n",
    "    print(\"DagsHub package not installed. Run 'pip install dagshub' to enable DagsHub integration.\")\n",
    "    dagshub_installed = False\n",
    "\n",
    "if dagshub_installed:\n",
    "    # Setup DagsHub integration\n",
    "    # Replace with your DagsHub username and repo\n",
    "    username = \"your-dagshub-username\"\n",
    "    repo_name = \"MLFlow_demo\"\n",
    "    \n",
    "    print(\"To configure DagsHub integration, you would run:\")\n",
    "    print(f\"dagshub.init(repo_owner='{username}', repo_name='{repo_name}', mlflow=True)\")\n",
    "    \n",
    "    # In a real scenario, you would uncomment and use the line below\n",
    "    # dagshub.init(repo_owner=username, repo_name=repo_name, mlflow=True)\n",
    "    \n",
    "    print(\"\\nBenefits of DagsHub integration:\")\n",
    "    print(\"1. Centralized experiment tracking accessible to all team members\")\n",
    "    print(\"2. Automated versioning of models and data\")\n",
    "    print(\"3. Collaborative model review process\")\n",
    "    print(\"4. Integrated CI/CD for MLOps workflows\")\n",
    "    print(\"5. Metrics visualization and sharing\")\n",
    "    \n",
    "    print(\"\\nTo share your MLflow experiments with your team, you would:\")\n",
    "    print(\"1. Push your code to the DagsHub repository\")\n",
    "    print(\"2. Share the DagsHub repository URL with your team\")\n",
    "    print(\"3. Team members can view experiments at https://dagshub.com/{username}/{repo_name}/experiments\")\n",
    "else:\n",
    "    print(\"\\nTo enable DagsHub integration, install the package:\")\n",
    "    print(\"pip install dagshub\")\n",
    "    print(\"\\nThen configure it in your scripts with:\")\n",
    "    print(\"import dagshub\")\n",
    "    print(\"dagshub.init(repo_owner='your-username', repo_name='your-repo', mlflow=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06eabd2",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated a complete MLOps workflow using MLflow for model training, tracking, and registry:\n",
    "\n",
    "1. **Setup**: We configured our MLflow tracking environment\n",
    "2. **Data Loading**: We loaded and prepared our dataset\n",
    "3. **Model Training**: We trained multiple models with hyperparameter tracking\n",
    "4. **Model Comparison**: We visualized and compared model performance\n",
    "5. **Model Registry**: We registered our best model and managed its lifecycle\n",
    "6. **Model Deployment**: We showed how to load and use registered models\n",
    "7. **Model Versioning**: We demonstrated version management and promotion\n",
    "8. **Collaboration**: We explored DagsHub integration for team collaboration\n",
    "\n",
    "This workflow provides a solid foundation for implementing MLOps practices in your organization, enabling reproducibility, collaboration, and effective model lifecycle management."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
