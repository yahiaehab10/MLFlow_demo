{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea67241",
   "metadata": {},
   "source": [
    "# Data Drift Detection with Evidently and MLflow\n",
    "\n",
    "This notebook demonstrates data drift detection using the Evidently library and MLflow tracking. We'll perform the following steps:\n",
    "\n",
    "1. Set up the environment and MLflow tracking with DagsHub\n",
    "2. Load the baseline data and trained model\n",
    "3. Generate synthetic data with drift\n",
    "4. Detect and visualize data drift\n",
    "5. Detect and visualize model performance drift\n",
    "6. Log drift reports to MLflow\n",
    "7. Create automated drift monitoring\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.11+\n",
    "- Required packages (scikit-learn, pandas, numpy, matplotlib, mlflow, dagshub, evidently)\n",
    "- Completed data cleaning notebook (01_data_cleaning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce08175",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up MLflow tracking with DagsHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce65b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# For visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Add parent directory to path to import project modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# For MLflow tracking\n",
    "try:\n",
    "    import mlflow\n",
    "    import dagshub\n",
    "    \n",
    "    # Initialize DagsHub with MLflow tracking\n",
    "    dagshub.init(\n",
    "        repo_owner='yahiaehab10', \n",
    "        repo_name='MLflow_demo_MF', \n",
    "        mlflow=True\n",
    "    )\n",
    "    \n",
    "    # Set experiment name\n",
    "    mlflow.set_experiment(\"drift_detection\")\n",
    "    \n",
    "    print(\"MLflow tracking with DagsHub initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not initialize DagsHub MLflow tracking: {e}\")\n",
    "    print(\"Continuing with default MLflow tracking.\")\n",
    "    import mlflow\n",
    "    mlflow.set_experiment(\"drift_detection\")\n",
    "\n",
    "# Import Evidently for drift detection\n",
    "try:\n",
    "    from evidently.report import Report\n",
    "    from evidently.metrics import DataDriftTable, ColumnDriftMetric, DatasetDriftMetric\n",
    "    from evidently.metrics import ColumnQuantileMetric, ColumnDistributionMetric\n",
    "    from evidently.metrics.regression_performance import RegressionQualityMetric, RegressionPredictedVsActualPlot\n",
    "    from evidently.metrics.classification_performance import ClassificationQualityMetric, ClassificationConfusionMatrix\n",
    "    \n",
    "    print(\"Evidently library imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error: Could not import Evidently library: {e}\")\n",
    "    print(\"Please install Evidently: pip install evidently\")\n",
    "\n",
    "# Set up paths\n",
    "data_dir = os.path.join('..', 'data')\n",
    "raw_dir = os.path.join(data_dir, 'raw')\n",
    "processed_dir = os.path.join(data_dir, 'processed')\n",
    "drift_baseline_dir = os.path.join(data_dir, 'drift_baseline')\n",
    "drift_results_dir = os.path.join(data_dir, 'drift_results')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(drift_results_dir, exist_ok=True)\n",
    "\n",
    "print(\"Environment setup completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e415b7",
   "metadata": {},
   "source": [
    "## 2. Load Baseline Data and Model\n",
    "\n",
    "Let's load the baseline data created in the previous notebook, as well as a simple trained model to test for model performance drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495df78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline statistics and data\n",
    "try:\n",
    "    baseline_stats = joblib.load(os.path.join(drift_baseline_dir, \"diabetes_baseline_stats.joblib\"))\n",
    "    baseline_data = pd.read_csv(os.path.join(drift_baseline_dir, \"diabetes_baseline_sample.csv\"))\n",
    "    print(\"Baseline data and statistics loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Baseline data or statistics not found. Please run the data cleaning notebook first.\")\n",
    "    baseline_stats = None\n",
    "    baseline_data = None\n",
    "\n",
    "# Load training and testing data\n",
    "try:\n",
    "    X_train = pd.read_csv(os.path.join(processed_dir, \"diabetes_X_train.csv\"))\n",
    "    X_test = pd.read_csv(os.path.join(processed_dir, \"diabetes_X_test.csv\"))\n",
    "    y_train = pd.read_csv(os.path.join(processed_dir, \"diabetes_y_train.csv\")).iloc[:, 0]\n",
    "    y_test = pd.read_csv(os.path.join(processed_dir, \"diabetes_y_test.csv\")).iloc[:, 0]\n",
    "    print(\"Training and testing data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Training or testing data not found. Please run the data cleaning notebook first.\")\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "# Train a simple model for performance drift demonstration\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "with mlflow.start_run(run_name=\"train_reference_model\"):\n",
    "    # Set model parameters\n",
    "    params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Log parameters\n",
    "    for param, value in params.items():\n",
    "        mlflow.log_param(param, value)\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(processed_dir, \"reference_model.joblib\")\n",
    "    joblib.dump(model, model_path)\n",
    "    mlflow.log_artifact(model_path)\n",
    "    \n",
    "    # Register model in Model Registry\n",
    "    mlflow.sklearn.log_model(\n",
    "        model, \n",
    "        artifact_path=\"reference_model\",\n",
    "        registered_model_name=\"diabetes_reference_model\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Reference model trained and saved with RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8faf5",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Data with Drift\n",
    "\n",
    "For demonstration purposes, we'll create synthetic data with different types of drift:\n",
    "\n",
    "1. **Concept Drift**: The relationship between input and output changes\n",
    "2. **Feature Drift**: The distribution of features changes\n",
    "3. **Data Quality Drift**: Missing values, outliers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate three types of synthetic data with drift\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Feature Drift: shift in the distribution of features\n",
    "def generate_feature_drift(X, columns_to_drift, drift_factor=0.5):\n",
    "    \"\"\"Generate data with feature drift by shifting the mean of selected features\"\"\"\n",
    "    X_drift = X.copy()\n",
    "    for col in columns_to_drift:\n",
    "        # Shift the mean by drift_factor standard deviations\n",
    "        shift = drift_factor * X[col].std()\n",
    "        X_drift[col] = X[col] + shift\n",
    "    return X_drift\n",
    "\n",
    "# 2. Covariate Drift: change in the distribution without changing relationships\n",
    "def generate_covariate_drift(X, noise_factor=0.3):\n",
    "    \"\"\"Generate data with covariate drift by adding noise to all features\"\"\"\n",
    "    X_drift = X.copy()\n",
    "    for col in X.columns:\n",
    "        # Add random noise\n",
    "        noise = np.random.normal(0, noise_factor * X[col].std(), size=len(X))\n",
    "        X_drift[col] = X[col] + noise\n",
    "    return X_drift\n",
    "\n",
    "# 3. Data Quality Drift: introduce missing values and outliers\n",
    "def generate_quality_drift(X, missing_pct=0.1, outlier_pct=0.05, outlier_factor=3):\n",
    "    \"\"\"Generate data with quality issues like missing values and outliers\"\"\"\n",
    "    X_drift = X.copy()\n",
    "    \n",
    "    # Introduce missing values\n",
    "    mask = np.random.choice([True, False], size=X.shape, p=[missing_pct, 1-missing_pct])\n",
    "    X_drift[mask] = np.nan\n",
    "    \n",
    "    # Introduce outliers\n",
    "    for col in X.columns:\n",
    "        # Select random indices for outliers\n",
    "        outlier_idx = np.random.choice(\n",
    "            X.index, \n",
    "            size=int(outlier_pct * len(X)), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        # Create outliers\n",
    "        X_drift.loc[outlier_idx, col] = X.loc[outlier_idx, col] * outlier_factor\n",
    "    \n",
    "    return X_drift\n",
    "\n",
    "# Generate the drift datasets\n",
    "columns_to_drift = ['bmi', 'bp', 's5']  # Example features to drift\n",
    "\n",
    "X_feature_drift = generate_feature_drift(X_test, columns_to_drift, drift_factor=0.8)\n",
    "X_covariate_drift = generate_covariate_drift(X_test, noise_factor=0.4)\n",
    "X_quality_drift = generate_quality_drift(X_test, missing_pct=0.1, outlier_pct=0.08, outlier_factor=4)\n",
    "\n",
    "# Save drift datasets\n",
    "X_feature_drift.to_csv(os.path.join(drift_results_dir, \"feature_drift_data.csv\"), index=False)\n",
    "X_covariate_drift.to_csv(os.path.join(drift_results_dir, \"covariate_drift_data.csv\"), index=False)\n",
    "X_quality_drift.to_csv(os.path.join(drift_results_dir, \"quality_drift_data.csv\"), index=False)\n",
    "\n",
    "# Fill missing values in quality drift dataset for visualization\n",
    "X_quality_drift_filled = X_quality_drift.fillna(X_quality_drift.mean())\n",
    "\n",
    "# Visualize the original data vs. the drift data\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot histograms for a few selected features to visualize drift\n",
    "features_to_plot = ['bmi', 'bp', 's5']\n",
    "drift_types = ['Feature Drift', 'Covariate Drift', 'Quality Drift']\n",
    "drift_data = [X_feature_drift, X_covariate_drift, X_quality_drift_filled]\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    for j, (drift_type, drift_dataset) in enumerate(zip(drift_types, drift_data)):\n",
    "        plt.subplot(len(features_to_plot), len(drift_types), i * len(drift_types) + j + 1)\n",
    "        \n",
    "        # Plot original data\n",
    "        sns.histplot(X_test[feature], color='blue', alpha=0.5, label='Original', kde=True)\n",
    "        \n",
    "        # Plot drift data\n",
    "        sns.histplot(drift_dataset[feature], color='red', alpha=0.5, label='Drift', kde=True)\n",
    "        \n",
    "        plt.title(f'{feature} - {drift_type}')\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "drift_viz_path = os.path.join(drift_results_dir, \"drift_visualization.png\")\n",
    "plt.savefig(drift_viz_path)\n",
    "plt.show()\n",
    "\n",
    "# Log the generated drift data to MLflow\n",
    "with mlflow.start_run(run_name=\"generate_drift_data\"):\n",
    "    mlflow.log_param(\"feature_drift_columns\", columns_to_drift)\n",
    "    mlflow.log_param(\"feature_drift_factor\", 0.8)\n",
    "    mlflow.log_param(\"covariate_drift_noise_factor\", 0.4)\n",
    "    mlflow.log_param(\"quality_drift_missing_pct\", 0.1)\n",
    "    mlflow.log_param(\"quality_drift_outlier_pct\", 0.08)\n",
    "    \n",
    "    # Log datasets as artifacts\n",
    "    mlflow.log_artifact(os.path.join(drift_results_dir, \"feature_drift_data.csv\"))\n",
    "    mlflow.log_artifact(os.path.join(drift_results_dir, \"covariate_drift_data.csv\"))\n",
    "    mlflow.log_artifact(os.path.join(drift_results_dir, \"quality_drift_data.csv\"))\n",
    "    \n",
    "    # Log visualization\n",
    "    mlflow.log_artifact(drift_viz_path)\n",
    "    \n",
    "    print(\"Drift data generated and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c67dd",
   "metadata": {},
   "source": [
    "## 4. Detect Data Drift with Evidently\n",
    "\n",
    "Now we'll use the Evidently library to detect and visualize the data drift between our original data and the synthetic drift data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fc235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to detect and analyze data drift\n",
    "def analyze_data_drift(reference_data, current_data, drift_type):\n",
    "    \"\"\"\n",
    "    Detect data drift between reference and current data\n",
    "    \n",
    "    Args:\n",
    "        reference_data: Original data (baseline)\n",
    "        current_data: New data to check for drift\n",
    "        drift_type: String describing the type of drift\n",
    "        \n",
    "    Returns:\n",
    "        report: Evidently report object\n",
    "        results: Dictionary with drift detection results\n",
    "    \"\"\"\n",
    "    # Create data drift report\n",
    "    data_drift_report = Report(metrics=[\n",
    "        DatasetDriftMetric(),\n",
    "        DataDriftTable(),\n",
    "    ])\n",
    "    \n",
    "    # Calculate drift\n",
    "    data_drift_report.run(reference_data=reference_data, current_data=current_data)\n",
    "    report_dict = data_drift_report.as_dict()\n",
    "    \n",
    "    # Extract drift results\n",
    "    results = {\n",
    "        'drift_detected': report_dict['metrics'][0]['result']['dataset_drift'],\n",
    "        'drift_score': report_dict['metrics'][0]['result']['dataset_drift_score'],\n",
    "        'number_of_drifted_columns': report_dict['metrics'][0]['result']['number_of_drifted_columns'],\n",
    "        'share_of_drifted_columns': report_dict['metrics'][0]['result']['share_of_drifted_columns'],\n",
    "        'column_drift_scores': {}\n",
    "    }\n",
    "    \n",
    "    # Extract column-level drift information\n",
    "    for col_result in report_dict['metrics'][1]['result']['columns'].values():\n",
    "        col_name = col_result['column_name']\n",
    "        results['column_drift_scores'][col_name] = {\n",
    "            'drift_detected': col_result['drift_detected'],\n",
    "            'drift_score': col_result['drift_score']\n",
    "        }\n",
    "    \n",
    "    # Save report as HTML\n",
    "    report_path = os.path.join(drift_results_dir, f\"{drift_type}_drift_report.html\")\n",
    "    data_drift_report.save_html(report_path)\n",
    "    \n",
    "    return data_drift_report, results, report_path\n",
    "\n",
    "# Analyze each type of drift\n",
    "drift_types = {\n",
    "    'feature': X_feature_drift,\n",
    "    'covariate': X_covariate_drift,\n",
    "    'quality': X_quality_drift.fillna(X_quality_drift.mean())  # Fill NaNs for analysis\n",
    "}\n",
    "\n",
    "# Start MLflow run for drift detection\n",
    "with mlflow.start_run(run_name=\"data_drift_detection\"):\n",
    "    all_results = {}\n",
    "    \n",
    "    for drift_type, drift_data in drift_types.items():\n",
    "        print(f\"\\nAnalyzing {drift_type} drift...\")\n",
    "        \n",
    "        # Detect drift\n",
    "        report, results, report_path = analyze_data_drift(X_test, drift_data, drift_type)\n",
    "        all_results[drift_type] = results\n",
    "        \n",
    "        # Log drift metrics to MLflow\n",
    "        mlflow.log_metric(f\"{drift_type}_drift_detected\", int(results['drift_detected']))\n",
    "        mlflow.log_metric(f\"{drift_type}_drift_score\", results['drift_score'])\n",
    "        mlflow.log_metric(f\"{drift_type}_drifted_columns_count\", results['number_of_drifted_columns'])\n",
    "        mlflow.log_metric(f\"{drift_type}_drifted_columns_share\", results['share_of_drifted_columns'])\n",
    "        \n",
    "        # Log most drifted columns\n",
    "        top_drifted = sorted(\n",
    "            [(col, info['drift_score']) for col, info in results['column_drift_scores'].items() if info['drift_detected']], \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )[:3]\n",
    "        \n",
    "        for i, (col, score) in enumerate(top_drifted):\n",
    "            mlflow.log_metric(f\"{drift_type}_top{i+1}_drifted_column_score\", score)\n",
    "            mlflow.log_param(f\"{drift_type}_top{i+1}_drifted_column\", col)\n",
    "        \n",
    "        # Log report as artifact\n",
    "        mlflow.log_artifact(report_path)\n",
    "        \n",
    "        # Print drift summary\n",
    "        print(f\"Drift detected: {results['drift_detected']}\")\n",
    "        print(f\"Drift score: {results['drift_score']:.4f}\")\n",
    "        print(f\"Number of drifted columns: {results['number_of_drifted_columns']} \" +\n",
    "              f\"({results['share_of_drifted_columns'] * 100:.1f}%)\")\n",
    "        \n",
    "        if results['drift_detected']:\n",
    "            print(\"\\nTop drifted columns:\")\n",
    "            for col, score in top_drifted:\n",
    "                print(f\"  - {col}: {score:.4f}\")\n",
    "    \n",
    "    # Save all results as JSON\n",
    "    all_results_path = os.path.join(drift_results_dir, \"all_drift_results.json\")\n",
    "    with open(all_results_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    mlflow.log_artifact(all_results_path)\n",
    "    \n",
    "    print(\"\\nData drift analysis completed and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919791a5",
   "metadata": {},
   "source": [
    "## 5. Detect Model Performance Drift\n",
    "\n",
    "Let's analyze how data drift affects the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e506c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to analyze model performance drift\n",
    "def analyze_model_performance_drift(model, reference_data, reference_targets, \n",
    "                                   current_data, current_targets, drift_type):\n",
    "    \"\"\"\n",
    "    Detect model performance drift\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        reference_data: Original data (baseline)\n",
    "        reference_targets: Original targets\n",
    "        current_data: New data to check for drift\n",
    "        current_targets: New targets\n",
    "        drift_type: String describing the type of drift\n",
    "        \n",
    "    Returns:\n",
    "        report: Evidently report object\n",
    "        results: Dictionary with performance drift results\n",
    "    \"\"\"\n",
    "    # Generate predictions\n",
    "    reference_predictions = model.predict(reference_data)\n",
    "    current_predictions = model.predict(current_data)\n",
    "    \n",
    "    # For regression\n",
    "    model_report = Report(metrics=[\n",
    "        RegressionQualityMetric(),\n",
    "        RegressionPredictedVsActualPlot()\n",
    "    ])\n",
    "    \n",
    "    # Prepare data\n",
    "    reference_data_with_pred = reference_data.copy()\n",
    "    reference_data_with_pred['target'] = reference_targets\n",
    "    reference_data_with_pred['prediction'] = reference_predictions\n",
    "    \n",
    "    current_data_with_pred = current_data.copy()\n",
    "    current_data_with_pred['target'] = current_targets\n",
    "    current_data_with_pred['prediction'] = current_predictions\n",
    "    \n",
    "    # Calculate performance drift\n",
    "    model_report.run(reference_data=reference_data_with_pred, \n",
    "                    current_data=current_data_with_pred,\n",
    "                    column_mapping={'target': 'target', 'prediction': 'prediction'})\n",
    "    \n",
    "    # Extract results\n",
    "    report_dict = model_report.as_dict()\n",
    "    reference_metrics = report_dict['metrics'][0]['result']['reference']\n",
    "    current_metrics = report_dict['metrics'][0]['result']['current']\n",
    "    \n",
    "    results = {\n",
    "        'reference': {\n",
    "            'rmse': reference_metrics['rmse'],\n",
    "            'mae': reference_metrics['mean_abs_error'],\n",
    "            'r2': reference_metrics['r2_score'],\n",
    "            'mape': reference_metrics['mean_abs_perc_error']\n",
    "        },\n",
    "        'current': {\n",
    "            'rmse': current_metrics['rmse'],\n",
    "            'mae': current_metrics['mean_abs_error'],\n",
    "            'r2': current_metrics['r2_score'],\n",
    "            'mape': current_metrics['mean_abs_perc_error']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate performance changes\n",
    "    for metric in results['reference'].keys():\n",
    "        results[f'{metric}_change'] = results['current'][metric] - results['reference'][metric]\n",
    "        results[f'{metric}_change_pct'] = (\n",
    "            (results['current'][metric] - results['reference'][metric]) / \n",
    "            abs(results['reference'][metric]) * 100 if results['reference'][metric] != 0 else float('inf')\n",
    "        )\n",
    "    \n",
    "    # Save report as HTML\n",
    "    report_path = os.path.join(drift_results_dir, f\"{drift_type}_performance_report.html\")\n",
    "    model_report.save_html(report_path)\n",
    "    \n",
    "    return model_report, results, report_path\n",
    "\n",
    "# Analyze model performance for each type of drift\n",
    "with mlflow.start_run(run_name=\"model_performance_drift\"):\n",
    "    performance_results = {}\n",
    "    \n",
    "    for drift_type, drift_data in drift_types.items():\n",
    "        print(f\"\\nAnalyzing model performance with {drift_type} drift...\")\n",
    "        \n",
    "        # Fill missing values if needed\n",
    "        if drift_type == 'quality':\n",
    "            drift_data_clean = X_quality_drift.copy()\n",
    "            for col in drift_data_clean.columns:\n",
    "                drift_data_clean[col] = drift_data_clean[col].fillna(X_test[col].mean())\n",
    "        else:\n",
    "            drift_data_clean = drift_data\n",
    "        \n",
    "        # Detect performance drift\n",
    "        report, results, report_path = analyze_model_performance_drift(\n",
    "            model, X_test, y_test, drift_data_clean, y_test, drift_type\n",
    "        )\n",
    "        performance_results[drift_type] = results\n",
    "        \n",
    "        # Log performance metrics to MLflow\n",
    "        mlflow.log_metric(f\"{drift_type}_reference_rmse\", results['reference']['rmse'])\n",
    "        mlflow.log_metric(f\"{drift_type}_current_rmse\", results['current']['rmse'])\n",
    "        mlflow.log_metric(f\"{drift_type}_rmse_change_pct\", results['rmse_change_pct'])\n",
    "        \n",
    "        mlflow.log_metric(f\"{drift_type}_reference_r2\", results['reference']['r2'])\n",
    "        mlflow.log_metric(f\"{drift_type}_current_r2\", results['current']['r2'])\n",
    "        mlflow.log_metric(f\"{drift_type}_r2_change_pct\", results['r2_change_pct'])\n",
    "        \n",
    "        # Log report as artifact\n",
    "        mlflow.log_artifact(report_path)\n",
    "        \n",
    "        # Print performance drift summary\n",
    "        print(f\"Original RMSE: {results['reference']['rmse']:.4f}\")\n",
    "        print(f\"Drift RMSE: {results['current']['rmse']:.4f}\")\n",
    "        print(f\"RMSE change: {results['rmse_change']:.4f} ({results['rmse_change_pct']:.2f}%)\")\n",
    "        \n",
    "        print(f\"Original R²: {results['reference']['r2']:.4f}\")\n",
    "        print(f\"Drift R²: {results['current']['r2']:.4f}\")\n",
    "        print(f\"R² change: {results['r2_change']:.4f} ({results['r2_change_pct']:.2f}%)\")\n",
    "    \n",
    "    # Save all performance results as JSON\n",
    "    perf_results_path = os.path.join(drift_results_dir, \"all_performance_results.json\")\n",
    "    with open(perf_results_path, 'w') as f:\n",
    "        json.dump(performance_results, f, indent=2)\n",
    "    mlflow.log_artifact(perf_results_path)\n",
    "    \n",
    "    # Create a summary visualization of performance changes\n",
    "    drift_types_list = list(performance_results.keys())\n",
    "    metrics = ['rmse', 'r2']\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        reference_values = [performance_results[dt]['reference'][metric] for dt in drift_types_list]\n",
    "        current_values = [performance_results[dt]['current'][metric] for dt in drift_types_list]\n",
    "        \n",
    "        # Create bar positions\n",
    "        bar_positions = np.arange(len(drift_types_list))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Create grouped bars\n",
    "        plt.bar(bar_positions - width/2, reference_values, width, label='Original', alpha=0.7)\n",
    "        plt.bar(bar_positions + width/2, current_values, width, label='Drift', alpha=0.7)\n",
    "        \n",
    "        # Add labels and title\n",
    "        plt.xlabel('Drift Type')\n",
    "        plt.ylabel(metric.upper())\n",
    "        plt.title(f'Impact of Drift on {metric.upper()}')\n",
    "        plt.xticks(bar_positions, drift_types_list)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels\n",
    "        for j, (ref, curr) in enumerate(zip(reference_values, current_values)):\n",
    "            plt.text(j - width/2, ref, f'{ref:.3f}', ha='center', va='bottom')\n",
    "            plt.text(j + width/2, curr, f'{curr:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save and log the visualization\n",
    "    perf_viz_path = os.path.join(drift_results_dir, \"performance_drift_visualization.png\")\n",
    "    plt.savefig(perf_viz_path)\n",
    "    mlflow.log_artifact(perf_viz_path)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nModel performance drift analysis completed and logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d6f6e",
   "metadata": {},
   "source": [
    "## 6. Setting Up Automated Drift Monitoring\n",
    "\n",
    "Let's create a simple function that can be used for automated drift monitoring in a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c31c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a monitoring function that can be used in production\n",
    "def monitor_drift(reference_data, current_data, model, reference_targets=None, current_targets=None,\n",
    "                 drift_threshold=0.1, performance_threshold=0.1, log_mlflow=True):\n",
    "    \"\"\"\n",
    "    Monitor data and performance drift in production\n",
    "    \n",
    "    Args:\n",
    "        reference_data: Baseline data\n",
    "        current_data: New production data\n",
    "        model: Trained model\n",
    "        reference_targets: Baseline targets (for performance monitoring)\n",
    "        current_targets: New targets (for performance monitoring)\n",
    "        drift_threshold: Threshold for data drift alerts\n",
    "        performance_threshold: Threshold for performance drift alerts\n",
    "        log_mlflow: Whether to log results to MLflow\n",
    "    \n",
    "    Returns:\n",
    "        dict: Monitoring results\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'timestamp': pd.Timestamp.now(),\n",
    "        'data_drift': {},\n",
    "        'performance_drift': None,\n",
    "        'alerts': []\n",
    "    }\n",
    "    \n",
    "    # Check for missing values in current data\n",
    "    missing_values = current_data.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(current_data)) * 100\n",
    "    \n",
    "    if missing_percentage.max() > 5:\n",
    "        results['alerts'].append(f\"High percentage of missing values detected: {missing_percentage.max():.2f}%\")\n",
    "    \n",
    "    # Create data drift report\n",
    "    data_drift_report = Report(metrics=[\n",
    "        DatasetDriftMetric(),\n",
    "        DataDriftTable(),\n",
    "    ])\n",
    "    \n",
    "    # Calculate drift\n",
    "    data_drift_report.run(reference_data=reference_data, current_data=current_data)\n",
    "    report_dict = data_drift_report.as_dict()\n",
    "    \n",
    "    # Extract drift results\n",
    "    results['data_drift'] = {\n",
    "        'drift_detected': report_dict['metrics'][0]['result']['dataset_drift'],\n",
    "        'drift_score': report_dict['metrics'][0]['result']['dataset_drift_score'],\n",
    "        'number_of_drifted_columns': report_dict['metrics'][0]['result']['number_of_drifted_columns'],\n",
    "        'share_of_drifted_columns': report_dict['metrics'][0]['result']['share_of_drifted_columns'],\n",
    "        'column_drift_scores': {}\n",
    "    }\n",
    "    \n",
    "    # Extract column-level drift information\n",
    "    for col_result in report_dict['metrics'][1]['result']['columns'].values():\n",
    "        col_name = col_result['column_name']\n",
    "        results['data_drift']['column_drift_scores'][col_name] = {\n",
    "            'drift_detected': col_result['drift_detected'],\n",
    "            'drift_score': col_result['drift_score']\n",
    "        }\n",
    "    \n",
    "    # Add data drift alerts\n",
    "    if results['data_drift']['drift_detected']:\n",
    "        results['alerts'].append(\n",
    "            f\"Data drift detected with score {results['data_drift']['drift_score']:.4f}\"\n",
    "        )\n",
    "        \n",
    "        # Get top drifted columns\n",
    "        top_drifted = sorted(\n",
    "            [(col, info['drift_score']) \n",
    "             for col, info in results['data_drift']['column_drift_scores'].items() \n",
    "             if info['drift_detected']], \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )[:3]\n",
    "        \n",
    "        if top_drifted:\n",
    "            drift_cols = \", \".join([f\"{col} ({score:.4f})\" for col, score in top_drifted])\n",
    "            results['alerts'].append(f\"Top drifted columns: {drift_cols}\")\n",
    "    \n",
    "    # Check model performance drift if targets are provided\n",
    "    if reference_targets is not None and current_targets is not None:\n",
    "        # Make predictions\n",
    "        reference_predictions = model.predict(reference_data)\n",
    "        current_predictions = model.predict(current_data)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import mean_squared_error, r2_score\n",
    "        \n",
    "        reference_rmse = np.sqrt(mean_squared_error(reference_targets, reference_predictions))\n",
    "        current_rmse = np.sqrt(mean_squared_error(current_targets, current_predictions))\n",
    "        \n",
    "        reference_r2 = r2_score(reference_targets, reference_predictions)\n",
    "        current_r2 = r2_score(current_targets, current_predictions)\n",
    "        \n",
    "        # Calculate changes\n",
    "        rmse_change = current_rmse - reference_rmse\n",
    "        rmse_change_pct = (rmse_change / reference_rmse) * 100 if reference_rmse != 0 else float('inf')\n",
    "        \n",
    "        r2_change = current_r2 - reference_r2\n",
    "        r2_change_pct = (r2_change / abs(reference_r2)) * 100 if reference_r2 != 0 else float('inf')\n",
    "        \n",
    "        # Store performance drift results\n",
    "        results['performance_drift'] = {\n",
    "            'reference_rmse': reference_rmse,\n",
    "            'current_rmse': current_rmse,\n",
    "            'rmse_change': rmse_change,\n",
    "            'rmse_change_pct': rmse_change_pct,\n",
    "            'reference_r2': reference_r2,\n",
    "            'current_r2': current_r2,\n",
    "            'r2_change': r2_change,\n",
    "            'r2_change_pct': r2_change_pct\n",
    "        }\n",
    "        \n",
    "        # Add performance drift alerts\n",
    "        if abs(rmse_change_pct) > performance_threshold * 100:\n",
    "            results['alerts'].append(\n",
    "                f\"Performance drift detected: RMSE changed by {rmse_change_pct:.2f}%\"\n",
    "            )\n",
    "        \n",
    "        if abs(r2_change) > performance_threshold:\n",
    "            results['alerts'].append(\n",
    "                f\"Performance drift detected: R² changed by {r2_change:.4f}\"\n",
    "            )\n",
    "    \n",
    "    # Log to MLflow if requested\n",
    "    if log_mlflow:\n",
    "        timestamp_str = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"drift_monitoring_{timestamp_str}\"):\n",
    "            # Log drift metrics\n",
    "            mlflow.log_metric(\"data_drift_detected\", int(results['data_drift']['drift_detected']))\n",
    "            mlflow.log_metric(\"data_drift_score\", results['data_drift']['drift_score'])\n",
    "            mlflow.log_metric(\"drifted_columns_count\", results['data_drift']['number_of_drifted_columns'])\n",
    "            \n",
    "            # Log performance metrics if available\n",
    "            if results['performance_drift'] is not None:\n",
    "                mlflow.log_metric(\"current_rmse\", results['performance_drift']['current_rmse'])\n",
    "                mlflow.log_metric(\"rmse_change_pct\", results['performance_drift']['rmse_change_pct'])\n",
    "                mlflow.log_metric(\"current_r2\", results['performance_drift']['current_r2'])\n",
    "                mlflow.log_metric(\"r2_change\", results['performance_drift']['r2_change'])\n",
    "            \n",
    "            # Log alerts as parameters\n",
    "            for i, alert in enumerate(results['alerts']):\n",
    "                mlflow.log_param(f\"alert_{i+1}\", alert)\n",
    "            \n",
    "            # Save and log results as JSON\n",
    "            monitor_results_path = os.path.join(drift_results_dir, f\"monitoring_results_{timestamp_str}.json\")\n",
    "            with open(monitor_results_path, 'w') as f:\n",
    "                # Convert timestamp to string for JSON serialization\n",
    "                results_json = results.copy()\n",
    "                results_json['timestamp'] = results_json['timestamp'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "                json.dump(results_json, f, indent=2)\n",
    "            \n",
    "            mlflow.log_artifact(monitor_results_path)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the monitoring function with our synthetic drift data\n",
    "print(\"Testing automated drift monitoring function...\")\n",
    "\n",
    "# Test with feature drift data\n",
    "monitoring_results = monitor_drift(\n",
    "    reference_data=X_test,\n",
    "    current_data=X_feature_drift,\n",
    "    model=model,\n",
    "    reference_targets=y_test,\n",
    "    current_targets=y_test,\n",
    "    drift_threshold=0.05,\n",
    "    performance_threshold=0.05,\n",
    "    log_mlflow=True\n",
    ")\n",
    "\n",
    "# Print monitoring results\n",
    "print(\"\\nMonitoring Results:\")\n",
    "print(f\"Timestamp: {monitoring_results['timestamp']}\")\n",
    "print(f\"Data drift detected: {monitoring_results['data_drift']['drift_detected']}\")\n",
    "print(f\"Data drift score: {monitoring_results['data_drift']['drift_score']:.4f}\")\n",
    "\n",
    "if monitoring_results['performance_drift'] is not None:\n",
    "    print(f\"RMSE change: {monitoring_results['performance_drift']['rmse_change_pct']:.2f}%\")\n",
    "    print(f\"R² change: {monitoring_results['performance_drift']['r2_change']:.4f}\")\n",
    "\n",
    "print(\"\\nAlerts:\")\n",
    "for alert in monitoring_results['alerts']:\n",
    "    print(f\"⚠️ {alert}\")\n",
    "\n",
    "# Save the monitoring function to a Python module\n",
    "monitoring_module = \"\"\"\n",
    "\\\"\\\"\\\"\n",
    "Drift monitoring module for production use.\n",
    "\\\"\\\"\\\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import DataDriftTable, DatasetDriftMetric\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import json\n",
    "import os\n",
    "\n",
    "def monitor_drift(reference_data, current_data, model, reference_targets=None, current_targets=None,\n",
    "                 drift_threshold=0.1, performance_threshold=0.1, log_mlflow=True, output_dir=None):\n",
    "    \\\"\\\"\\\"\n",
    "    Monitor data and performance drift in production\n",
    "    \n",
    "    Args:\n",
    "        reference_data: Baseline data\n",
    "        current_data: New production data\n",
    "        model: Trained model\n",
    "        reference_targets: Baseline targets (for performance monitoring)\n",
    "        current_targets: New targets (for performance monitoring)\n",
    "        drift_threshold: Threshold for data drift alerts\n",
    "        performance_threshold: Threshold for performance drift alerts\n",
    "        log_mlflow: Whether to log results to MLflow\n",
    "        output_dir: Directory to save results\n",
    "    \n",
    "    Returns:\n",
    "        dict: Monitoring results\n",
    "    \\\"\\\"\\\"\n",
    "    results = {\n",
    "        'timestamp': pd.Timestamp.now(),\n",
    "        'data_drift': {},\n",
    "        'performance_drift': None,\n",
    "        'alerts': []\n",
    "    }\n",
    "    \n",
    "    # Check for missing values in current data\n",
    "    missing_values = current_data.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(current_data)) * 100\n",
    "    \n",
    "    if missing_percentage.max() > 5:\n",
    "        results['alerts'].append(f\"High percentage of missing values detected: {missing_percentage.max():.2f}%\")\n",
    "    \n",
    "    # Create data drift report\n",
    "    data_drift_report = Report(metrics=[\n",
    "        DatasetDriftMetric(),\n",
    "        DataDriftTable(),\n",
    "    ])\n",
    "    \n",
    "    # Calculate drift\n",
    "    data_drift_report.run(reference_data=reference_data, current_data=current_data)\n",
    "    report_dict = data_drift_report.as_dict()\n",
    "    \n",
    "    # Extract drift results\n",
    "    results['data_drift'] = {\n",
    "        'drift_detected': report_dict['metrics'][0]['result']['dataset_drift'],\n",
    "        'drift_score': report_dict['metrics'][0]['result']['dataset_drift_score'],\n",
    "        'number_of_drifted_columns': report_dict['metrics'][0]['result']['number_of_drifted_columns'],\n",
    "        'share_of_drifted_columns': report_dict['metrics'][0]['result']['share_of_drifted_columns'],\n",
    "        'column_drift_scores': {}\n",
    "    }\n",
    "    \n",
    "    # Extract column-level drift information\n",
    "    for col_result in report_dict['metrics'][1]['result']['columns'].values():\n",
    "        col_name = col_result['column_name']\n",
    "        results['data_drift']['column_drift_scores'][col_name] = {\n",
    "            'drift_detected': col_result['drift_detected'],\n",
    "            'drift_score': col_result['drift_score']\n",
    "        }\n",
    "    \n",
    "    # Add data drift alerts\n",
    "    if results['data_drift']['drift_detected']:\n",
    "        results['alerts'].append(\n",
    "            f\"Data drift detected with score {results['data_drift']['drift_score']:.4f}\"\n",
    "        )\n",
    "        \n",
    "        # Get top drifted columns\n",
    "        top_drifted = sorted(\n",
    "            [(col, info['drift_score']) \n",
    "             for col, info in results['data_drift']['column_drift_scores'].items() \n",
    "             if info['drift_detected']], \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )[:3]\n",
    "        \n",
    "        if top_drifted:\n",
    "            drift_cols = \", \".join([f\"{col} ({score:.4f})\" for col, score in top_drifted])\n",
    "            results['alerts'].append(f\"Top drifted columns: {drift_cols}\")\n",
    "    \n",
    "    # Check model performance drift if targets are provided\n",
    "    if reference_targets is not None and current_targets is not None:\n",
    "        # Make predictions\n",
    "        reference_predictions = model.predict(reference_data)\n",
    "        current_predictions = model.predict(current_data)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        reference_rmse = np.sqrt(mean_squared_error(reference_targets, reference_predictions))\n",
    "        current_rmse = np.sqrt(mean_squared_error(current_targets, current_predictions))\n",
    "        \n",
    "        reference_r2 = r2_score(reference_targets, reference_predictions)\n",
    "        current_r2 = r2_score(current_targets, current_predictions)\n",
    "        \n",
    "        # Calculate changes\n",
    "        rmse_change = current_rmse - reference_rmse\n",
    "        rmse_change_pct = (rmse_change / reference_rmse) * 100 if reference_rmse != 0 else float('inf')\n",
    "        \n",
    "        r2_change = current_r2 - reference_r2\n",
    "        r2_change_pct = (r2_change / abs(reference_r2)) * 100 if reference_r2 != 0 else float('inf')\n",
    "        \n",
    "        # Store performance drift results\n",
    "        results['performance_drift'] = {\n",
    "            'reference_rmse': reference_rmse,\n",
    "            'current_rmse': current_rmse,\n",
    "            'rmse_change': rmse_change,\n",
    "            'rmse_change_pct': rmse_change_pct,\n",
    "            'reference_r2': reference_r2,\n",
    "            'current_r2': current_r2,\n",
    "            'r2_change': r2_change,\n",
    "            'r2_change_pct': r2_change_pct\n",
    "        }\n",
    "        \n",
    "        # Add performance drift alerts\n",
    "        if abs(rmse_change_pct) > performance_threshold * 100:\n",
    "            results['alerts'].append(\n",
    "                f\"Performance drift detected: RMSE changed by {rmse_change_pct:.2f}%\"\n",
    "            )\n",
    "        \n",
    "        if abs(r2_change) > performance_threshold:\n",
    "            results['alerts'].append(\n",
    "                f\"Performance drift detected: R² changed by {r2_change:.4f}\"\n",
    "            )\n",
    "    \n",
    "    # Log to MLflow if requested\n",
    "    if log_mlflow:\n",
    "        timestamp_str = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"drift_monitoring_{timestamp_str}\"):\n",
    "            # Log drift metrics\n",
    "            mlflow.log_metric(\"data_drift_detected\", int(results['data_drift']['drift_detected']))\n",
    "            mlflow.log_metric(\"data_drift_score\", results['data_drift']['drift_score'])\n",
    "            mlflow.log_metric(\"drifted_columns_count\", results['data_drift']['number_of_drifted_columns'])\n",
    "            \n",
    "            # Log performance metrics if available\n",
    "            if results['performance_drift'] is not None:\n",
    "                mlflow.log_metric(\"current_rmse\", results['performance_drift']['current_rmse'])\n",
    "                mlflow.log_metric(\"rmse_change_pct\", results['performance_drift']['rmse_change_pct'])\n",
    "                mlflow.log_metric(\"current_r2\", results['performance_drift']['current_r2'])\n",
    "                mlflow.log_metric(\"r2_change\", results['performance_drift']['r2_change'])\n",
    "            \n",
    "            # Log alerts as parameters\n",
    "            for i, alert in enumerate(results['alerts']):\n",
    "                mlflow.log_param(f\"alert_{i+1}\", alert)\n",
    "            \n",
    "            # Save and log results as JSON\n",
    "            if output_dir is None:\n",
    "                output_dir = 'drift_results'\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                \n",
    "            monitor_results_path = os.path.join(output_dir, f\"monitoring_results_{timestamp_str}.json\")\n",
    "            with open(monitor_results_path, 'w') as f:\n",
    "                # Convert timestamp to string for JSON serialization\n",
    "                results_json = results.copy()\n",
    "                results_json['timestamp'] = results_json['timestamp'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "                json.dump(results_json, f, indent=2)\n",
    "            \n",
    "            mlflow.log_artifact(monitor_results_path)\n",
    "    \n",
    "    return results\n",
    "\"\"\"\n",
    "\n",
    "# Save the monitoring module\n",
    "with open(os.path.join('..', 'src', 'drift_monitoring.py'), 'w') as f:\n",
    "    f.write(monitoring_module)\n",
    "\n",
    "print(\"\\nDrift monitoring module saved to '../src/drift_monitoring.py'\")\n",
    "print(\"This module can be imported and used for automated drift monitoring in production.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d074f4",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In this notebook, we've:\n",
    "\n",
    "1. Set up the environment and MLflow tracking with DagsHub\n",
    "2. Loaded the baseline data and trained a simple model\n",
    "3. Generated synthetic data with different types of drift\n",
    "4. Detected and visualized data drift using Evidently\n",
    "5. Analyzed the impact of drift on model performance\n",
    "6. Created an automated drift monitoring solution for production use\n",
    "\n",
    "All the drift reports and visualizations are logged to MLflow and can be viewed in the MLflow UI. The monitoring function we created can be used in a production environment to continuously monitor for drift and alert when necessary.\n",
    "\n",
    "Next steps:\n",
    "- Proceed to model training notebook (03_model_training.ipynb)\n",
    "- Implement the drift monitoring in a production pipeline"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
